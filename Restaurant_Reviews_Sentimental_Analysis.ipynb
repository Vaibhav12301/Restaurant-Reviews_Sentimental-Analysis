{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaibhav12301/Restaurant-Reviews_Sentimental-Analysis/blob/main/Restaurant_Reviews_Sentimental_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aytW8-RYPdjd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71b584c8-8fe7-4420-f810-00a22d006e15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from string import punctuation\n",
        "from sklearn import svm\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from nltk import ngrams\n",
        "from itertools import chain\n",
        "from wordcloud import WordCloud\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "1Ly1BG5zgo5F",
        "outputId": "fd30b814-2833-43cc-f48a-8a36b9ec9769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQaWbqgncCa1"
      },
      "outputs": [],
      "source": [
        "odf = pd.read_csv('/content/drive/MyDrive/Reviews.csv')\n",
        "odf['Helpful %'] = np.where(odf['HelpfulnessDenominator'] > 0, odf['HelpfulnessNumerator'] / odf['HelpfulnessDenominator'], -1)\n",
        "odf['% Upvote'] = pd.cut(odf['Helpful %'], bins = [-1, 0, 0.2, 0.4, 0.6, 0.8, 1.0], labels = ['Empty', '0-20%', '20-40%', '40-60%', '60-80%', '80-100%'], include_lowest = True)\n",
        "odf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULWMkg6Iem8u"
      },
      "outputs": [],
      "source": [
        "df_s = odf.groupby(['Score', '% Upvote']).agg({'Id': 'count'})\n",
        "df_s = df_s.unstack()\n",
        "df_s.columns = df_s.columns.get_level_values(1)\n",
        "fig = plt.figure(figsize=(15,10))\n",
        "\n",
        "sns.heatmap(df_s[df_s.columns[::-1]].T, cmap = \"YlGnBu\", linewidths =.5, annot = True, fmt = 'd', cbar_kws={'label': '# reviews'})\n",
        "plt.yticks(rotation=0)\n",
        "plt.title('How helpful users find among user scores')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk457n1yfIn3"
      },
      "source": [
        "Key message from above:\n",
        "\n",
        "Reviews are skewed towards positive\n",
        "More than half of the reviews are with zero votes\n",
        "Many people agree with score 5 reviews\n",
        "Then we get rid of score 3 reviews(Neutral), and separate the remaining reviews into binary class(1 = positive, 0 = negative):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biJJ5TmWfXTK"
      },
      "outputs": [],
      "source": [
        "df = odf[odf['Score'] !=3]\n",
        "X = df['Text']\n",
        "y_dict = {1:0, 2:0, 4:1, 5:1}\n",
        "y = df['Score'].map(y_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj-r27W7fi1G"
      },
      "source": [
        "# **Score Prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh1uNgP2fswf"
      },
      "source": [
        "Logistic Regression model on word count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGkQlH9Wf8Mv"
      },
      "outputs": [],
      "source": [
        "c = CountVectorizer(stop_words = 'english')\n",
        "\n",
        "def text_fit(X, y, model,clf_model,coef_show=1):\n",
        "\n",
        "    X_c = model.fit_transform(X)\n",
        "    print('# features: {}'.format(X_c.shape[1]))\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_c, y, random_state=0)\n",
        "    print('# train records: {}'.format(X_train.shape[0]))\n",
        "    print('# test records: {}'.format(X_test.shape[0]))\n",
        "    clf = clf_model.fit(X_train, y_train)\n",
        "    acc = clf.score(X_test, y_test)\n",
        "    print ('Model Accuracy: {}'.format(acc))\n",
        "\n",
        "    if coef_show == 1:\n",
        "        w = model.get_feature_names_out()\n",
        "        coef = clf.coef_.tolist()[0]\n",
        "        coeff_df = pd.DataFrame({'Word' : w, 'Coefficient' : coef})\n",
        "        coeff_df = coeff_df.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
        "        print('')\n",
        "        print('-Top 20 positive-')\n",
        "        print(coeff_df.head(20).to_string(index=False))\n",
        "        print('')\n",
        "        print('-Top 20 negative-')\n",
        "        print(coeff_df.tail(20).to_string(index=False))\n",
        "\n",
        "\n",
        "text_fit(X, y, c, LogisticRegression())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jreVuQd0gr0r"
      },
      "source": [
        "Accuracy is around 93.9% - not bad. However we notice that some of those significant coefficient are not meaningful.\n",
        "\n",
        "Let's also look at the base line accuracy (predicting with majority class, in this case positive class):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI_RsNOBhGjo"
      },
      "source": [
        "**QUICK EDA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSM8HG8NhTNy"
      },
      "outputs": [],
      "source": [
        "ax = df['Score'].value_counts().sort_index() \\\n",
        "    .plot(kind='bar',\n",
        "          title='Count of Reviews by Stars',\n",
        "          figsize=(10, 5))\n",
        "ax.set_xlabel('Review Stars')\n",
        "plt.show()\n",
        "#data grapt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo9fVcRdhea1"
      },
      "source": [
        "**Basic NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeLgSMydhlaX"
      },
      "outputs": [],
      "source": [
        "example = df['Text'][50]\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tI43QEdDiBVq"
      },
      "outputs": [],
      "source": [
        "tokens = nltk.word_tokenize(example)\n",
        "tokens[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NUvpUf6iSHJ"
      },
      "outputs": [],
      "source": [
        "tagged = nltk.pos_tag(tokens)\n",
        "tagged[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08y9NmsiivQl"
      },
      "outputs": [],
      "source": [
        "entities = nltk.chunk.ne_chunk(tagged)\n",
        "entities.pprint()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdHpIQCulHTC"
      },
      "source": [
        "# **Step 1. VADER Seniment Scoring**\n",
        "We will use NLTK's SentimentIntensityAnalyzer to get the neg/neu/pos scores of the text.\n",
        "\n",
        "This uses a \"bag of words\" approach:\n",
        "1. Stop words are removed.\n",
        "2. Each word is scored and combined to a total score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gx1c4lBWlMoz"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymvLy4islYSk"
      },
      "outputs": [],
      "source": [
        "sia.polarity_scores('I am so happy!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DM-3v1YPl4oM"
      },
      "outputs": [],
      "source": [
        "sia.polarity_scores('This is the worst thing ever.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eubfsr2ll96h"
      },
      "outputs": [],
      "source": [
        "sia.polarity_scores(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooLihzD3mg1j"
      },
      "outputs": [],
      "source": [
        "# Run the polarity score on the entire dataset\n",
        "res = {}\n",
        "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    text = row['Text']\n",
        "    myid = row['Id']\n",
        "    res[myid] = sia.polarity_scores(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylP6AYfYpoLH"
      },
      "outputs": [],
      "source": [
        "vaders = pd.DataFrame(res).T\n",
        "vaders = vaders.reset_index().rename(columns={'index': 'Id'})\n",
        "vaders = vaders.merge(df, how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uraa5NNOtDVQ"
      },
      "outputs": [],
      "source": [
        "# Now we have sentiment score and metadata\n",
        "vaders.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqPq_hlUtY7-"
      },
      "source": [
        "Plot **VADER results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYUVyZ8Gtcto"
      },
      "outputs": [],
      "source": [
        "ax = sns.barplot(data=vaders, x='Score', y='compound')\n",
        "ax.set_title('Compund Score ')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jaRCfs4uQdQ"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 3, figsize=(12, 3))\n",
        "sns.barplot(data=vaders, x='Score', y='pos', ax=axs[0])\n",
        "sns.barplot(data=vaders, x='Score', y='neu', ax=axs[1])\n",
        "sns.barplot(data=vaders, x='Score', y='neg', ax=axs[2])\n",
        "axs[0].set_title('Positive')\n",
        "axs[1].set_title('Neutral')\n",
        "axs[2].set_title('Negative')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExNiMivavBZy"
      },
      "source": [
        "# **Step 2. Roberta Pretrained Model**\n",
        "\n",
        "*   Use a model trained of a large corpus of data.\n",
        "*   Transformer model accounts for the words but also the context related to other **words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7CNnnaBvEdF"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN-KKnp0vSZJ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from scipy.special import softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np9iCIY5v1AG"
      },
      "outputs": [],
      "source": [
        "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aW3METikwT5w"
      },
      "outputs": [],
      "source": [
        "# VADER results on example\n",
        "print(example)\n",
        "sia.polarity_scores(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cy92Wj5awb2C"
      },
      "outputs": [],
      "source": [
        "# Run for Roberta Model\n",
        "encoded_text = tokenizer(example, return_tensors='pt')\n",
        "output = model(**encoded_text)\n",
        "scores = output[0][0].detach().numpy()\n",
        "scores = softmax(scores)\n",
        "scores_dict = {\n",
        "    'roberta_neg' : scores[0],\n",
        "    'roberta_neu' : scores[1],\n",
        "    'roberta_pos' : scores[2]\n",
        "}\n",
        "print(scores_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JyWy8V7w4C0"
      },
      "outputs": [],
      "source": [
        "def polarity_scores_roberta(example):\n",
        "    encoded_text = tokenizer(example, return_tensors='pt')\n",
        "    output = model(**encoded_text)\n",
        "    scores = output[0][0].detach().numpy()\n",
        "    scores = softmax(scores)\n",
        "    scores_dict = {\n",
        "        'roberta_neg' : scores[0],\n",
        "        'roberta_neu' : scores[1],\n",
        "        'roberta_pos' : scores[2]\n",
        "    }\n",
        "    return scores_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0MLz74I85cX"
      },
      "outputs": [],
      "source": [
        "res = {}\n",
        "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    try:\n",
        "        text = row['Text']\n",
        "        myid = row['Id']\n",
        "        vader_result = sia.polarity_scores(text)\n",
        "        vader_result_rename = {}\n",
        "        for key, value in vader_result.items():\n",
        "            vader_result_rename[f\"vader_{key}\"] = value\n",
        "        roberta_result = polarity_scores_roberta(text)\n",
        "        both = {**vader_result_rename, **roberta_result}\n",
        "        res[myid] = both\n",
        "    except RuntimeError:\n",
        "        print(f'Broke for id {myid}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNGDjxh_-VSs"
      },
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(res).T\n",
        "results_df = results_df.reset_index().rename(columns={'index': 'Id'})\n",
        "results_df = results_df.merge(df, how='left')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgTiCGPD-ZZL"
      },
      "source": [
        "**Compare Scores between models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZY2idS8EAS-T"
      },
      "outputs": [],
      "source": [
        "print(results_df.columns)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}